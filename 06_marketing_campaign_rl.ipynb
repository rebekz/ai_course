{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Reinforcement Learning for Marketing Campaign Optimization\n",
    "\n",
    "In this notebook, you'll learn how to apply policy gradient reinforcement learning to optimize marketing budget allocation across multiple channels. You'll implement a simulated marketing environment, train an RL agent that maximizes campaign ROI, and analyze how the learned policy adapts to changing market conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reinforcement Learning Concept Refresher\n",
    "\n",
    "### The RL Loop\n",
    "- **Agent** interacts with an **Environment** through **Actions**\n",
    "- Environment returns **State** and **Reward**\n",
    "- Agent aims to maximize cumulative reward over time\n",
    "\n",
    "### Policy Gradients\n",
    "- **Policy**: Function \u03c0(a|s) that outputs probability distribution over actions\n",
    "- **Objective**: Maximize expected return J(\u03b8) = E[\u2211 rewards]\n",
    "- **Update rule**: \u03b8 \u2190 \u03b8 + \u03b1\u2207J(\u03b8)\n",
    "\n",
    "### Why Policy Gradients for Marketing?\n",
    "- Naturally handles **continuous action spaces** (budget allocations)\n",
    "- Works well with **stochastic environments** (uncertain market responses)\n",
    "- Can learn **complex allocation strategies** without manual rules\n",
    "- Adapts to **delayed rewards** (customer journey often spans days/weeks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our simulated marketing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marketing Campaign Environment\n",
    "\n",
    "We'll implement a marketing environment with the following characteristics:\n",
    "- 3 marketing channels: Email, Social Media, and Search Ads\n",
    "- Daily budget allocation decisions\n",
    "- Stochastic returns with diminishing returns\n",
    "- Delayed rewards (conversions happen over time)\n",
    "- Market saturation effects (effectiveness changes over time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketingEnvironment:\n",
    "    def __init__(self, num_channels=3, max_budget=1000, episode_length=30):\n",
    "        self.num_channels = num_channels\n",
    "        self.max_budget = max_budget\n",
    "        self.episode_length = episode_length\n",
    "        \n",
    "        # Channel characteristics\n",
    "        # [Email, Social, Search]\n",
    "        self.base_effectiveness = np.array([0.3, 0.5, 0.7])  # Base return per dollar\n",
    "        self.saturation_points = np.array([300, 500, 400])   # Saturation points (diminishing returns)\n",
    "        self.volatility = np.array([0.1, 0.2, 0.15])         # Daily effectiveness volatility\n",
    "        \n",
    "        # Conversion delay model (days until conversion)\n",
    "        self.delay_probs = np.array([0.5, 0.3, 0.15, 0.05])  # Probability of conversion after 0,1,2,3 days\n",
    "        \n",
    "        # State variables\n",
    "        self.day = 0\n",
    "        self.current_effectiveness = self.base_effectiveness.copy()\n",
    "        self.pending_conversions = deque(maxlen=len(self.delay_probs))\n",
    "        [self.pending_conversions.append(np.zeros(self.num_channels)) for _ in range(len(self.delay_probs))]\n",
    "        \n",
    "        # Tracking metrics\n",
    "        self.total_reward = 0\n",
    "        self.channel_spend = np.zeros(self.num_channels)\n",
    "        self.channel_revenue = np.zeros(self.num_channels)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment for a new episode\"\"\"\n",
    "        self.day = 0\n",
    "        self.current_effectiveness = self.base_effectiveness.copy()\n",
    "        self.pending_conversions = deque(maxlen=len(self.delay_probs))\n",
    "        [self.pending_conversions.append(np.zeros(self.num_channels)) for _ in range(len(self.delay_probs))]\n",
    "        self.total_reward = 0\n",
    "        self.channel_spend = np.zeros(self.num_channels)\n",
    "        self.channel_revenue = np.zeros(self.num_channels)\n",
    "        \n",
    "        # Initial state: [day/episode_length, effectiveness_ch1, effectiveness_ch2, effectiveness_ch3,\n",
    "        #                recent_spend_ch1, recent_spend_ch2, recent_spend_ch3,\n",
    "        #                pending_conversions_0, pending_conversions_1, ...]\n",
    "        state = self._get_state()\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the environment given an action\n",
    "        \n",
    "        Args:\n",
    "            action: numpy array of shape (num_channels,) with budget allocations\n",
    "                    Values should be between 0 and 1, will be scaled to max_budget\n",
    "        \n",
    "        Returns:\n",
    "            next_state: Current state representation\n",
    "            reward: Reward from this step\n",
    "            done: Whether episode is finished\n",
    "            info: Additional information dictionary\n",
    "        \"\"\"\n",
    "        # Scale action to actual budget\n",
    "        # Ensure action is valid\n",
    "        action = np.clip(action, 0, 1)\n",
    "        # Normalize to sum to 1 (if not already)\n",
    "        if np.sum(action) > 0:\n",
    "            action = action / np.sum(action)\n",
    "        \n",
    "        # Scale to max budget\n",
    "        budget_allocation = action * self.max_budget\n",
    "        \n",
    "        # Update spend tracking\n",
    "        self.channel_spend += budget_allocation\n",
    "        \n",
    "        # Calculate immediate returns based on channel effectiveness and diminishing returns\n",
    "        channel_returns = []\n",
    "        for i in range(self.num_channels):\n",
    "            # Apply diminishing returns formula: return = effectiveness * spend * exp(-spend/saturation)\n",
    "            spend = budget_allocation[i]\n",
    "            effectiveness = self.current_effectiveness[i]\n",
    "            saturation = self.saturation_points[i]\n",
    "            \n",
    "            # Calculate expected return before random variation\n",
    "            expected_return = effectiveness * spend * np.exp(-spend / saturation)\n",
    "            \n",
    "            # Add stochasticity\n",
    "            actual_return = expected_return * (1 + np.random.normal(0, 0.2))\n",
    "            channel_returns.append(max(0, actual_return))\n",
    "        \n",
    "        channel_returns = np.array(channel_returns)\n",
    "        \n",
    "        # Distribute returns across delayed conversion timeline\n",
    "        for day_offset, prob in enumerate(self.delay_probs):\n",
    "            self.pending_conversions[day_offset] += channel_returns * prob\n",
    "        \n",
    "        # Collect immediate reward (conversions from current and previous days)\n",
    "        immediate_reward = np.sum(self.pending_conversions[0])\n",
    "        \n",
    "        # Update channel revenue tracking\n",
    "        self.channel_revenue += self.pending_conversions[0]\n",
    "        \n",
    "        # Update total reward\n",
    "        self.total_reward += immediate_reward\n",
    "        \n",
    "        # Rotate pending conversions queue\n",
    "        self.pending_conversions.append(np.zeros(self.num_channels))\n",
    "        \n",
    "        # Update effectiveness (market dynamics)\n",
    "        self._update_market_dynamics()\n",
    "        \n",
    "        # Advance to next day\n",
    "        self.day += 1\n",
    "        done = self.day >= self.episode_length\n",
    "        \n",
    "        # Get new state\n",
    "        next_state = self._get_state()\n",
    "        \n",
    "        # Calculate ROI for info\n",
    "        total_spend = np.sum(budget_allocation)\n",
    "        roi = (immediate_reward - total_spend) / total_spend if total_spend > 0 else 0\n",
    "        \n",
    "        info = {\n",
    "            'spend': budget_allocation,\n",
    "            'returns': channel_returns,\n",
    "            'immediate_reward': immediate_reward,\n",
    "            'roi': roi,\n",
    "            'effectiveness': self.current_effectiveness.copy()\n",
    "        }\n",
    "        \n",
    "        return next_state, immediate_reward, done, info\n",
    "    \n",
    "    def _update_market_dynamics(self):\n",
    "        \"\"\"Update channel effectiveness based on market dynamics\"\"\"\n",
    "        # Random walk with mean reversion\n",
    "        for i in range(self.num_channels):\n",
    "            # Mean reversion factor (pulls back toward base effectiveness)\n",
    "            mean_reversion = 0.1 * (self.base_effectiveness[i] - self.current_effectiveness[i])\n",
    "            \n",
    "            # Random variation based on volatility\n",
    "            random_change = np.random.normal(0, self.volatility[i])\n",
    "            \n",
    "            # Update effectiveness\n",
    "            self.current_effectiveness[i] += mean_reversion + random_change\n",
    "            \n",
    "            # Ensure effectiveness stays positive\n",
    "            self.current_effectiveness[i] = max(0.1, self.current_effectiveness[i])\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Return current state representation\"\"\"\n",
    "        # Normalized day\n",
    "        normalized_day = self.day / self.episode_length\n",
    "        \n",
    "        # Channel effectiveness (normalized)\n",
    "        normalized_effectiveness = self.current_effectiveness / np.max(self.base_effectiveness)\n",
    "        \n",
    "        # Recent spend (normalized to max budget)\n",
    "        normalized_recent_spend = np.zeros(self.num_channels)\n",
    "        if self.day > 0:\n",
    "            recent_spend = self.channel_spend / (self.day * self.max_budget)\n",
    "            normalized_recent_spend = recent_spend\n",
    "        \n",
    "        # Flatten pending conversions\n",
    "        flattened_pending = np.array([np.sum(conv) for conv in list(self.pending_conversions)[1:]]) / self.max_budget\n",
    "        \n",
    "        # Combine all state components\n",
    "        state = np.concatenate([\n",
    "            [normalized_day],\n",
    "            normalized_effectiveness,\n",
    "            normalized_recent_spend,\n",
    "            flattened_pending\n",
    "        ])\n",
    "        \n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our environment with a simple run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence index must be integer, not 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m env = MarketingEnvironment()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m state = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mState shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInitial state: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mMarketingEnvironment.reset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mself\u001b[39m.channel_revenue = np.zeros(\u001b[38;5;28mself\u001b[39m.num_channels)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Initial state: [day/episode_length, effectiveness_ch1, effectiveness_ch2, effectiveness_ch3,\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m#                recent_spend_ch1, recent_spend_ch2, recent_spend_ch3,\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m#                pending_conversions_0, pending_conversions_1, ...]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 157\u001b[39m, in \u001b[36mMarketingEnvironment._get_state\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    154\u001b[39m     normalized_recent_spend = recent_spend\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# Flatten pending conversions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m flattened_pending = np.array([np.sum(conv) \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpending_conversions\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m]) / \u001b[38;5;28mself\u001b[39m.max_budget\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# Combine all state components\u001b[39;00m\n\u001b[32m    160\u001b[39m state = np.concatenate([\n\u001b[32m    161\u001b[39m     [normalized_day],\n\u001b[32m    162\u001b[39m     normalized_effectiveness,\n\u001b[32m    163\u001b[39m     normalized_recent_spend,\n\u001b[32m    164\u001b[39m     flattened_pending\n\u001b[32m    165\u001b[39m ])\n",
      "\u001b[31mTypeError\u001b[39m: sequence index must be integer, not 'slice'"
     ]
    }
   ],
   "source": [
    "env = MarketingEnvironment()\n",
    "state = env.reset()\n",
    "\n",
    "print(f\"State shape: {state.shape}\")\n",
    "print(f\"Initial state: {state}\")\n",
    "\n",
    "# Try a random action\n",
    "action = np.random.rand(3)\n",
    "next_state, reward, done, info = env.step(action)\n",
    "\n",
    "print(f\"\\nAction taken: {action}\")\n",
    "print(f\"Scaled budget allocation: {info['spend']}\")\n",
    "print(f\"Channel returns: {info['returns']}\")\n",
    "print(f\"Immediate reward: {reward}\")\n",
    "print(f\"ROI: {info['roi']:.2f}\")\n",
    "print(f\"Next state: {next_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline Agents\n",
    "\n",
    "Before implementing our policy gradient agent, let's establish some baselines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def run_episode(env, policy_fn, render=False):\n",
    "    \"\"\"Run a full episode using the provided policy function\"\"\"\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    episode_data = []\n",
    "    \n",
    "    while not done:\n",
    "        action = policy_fn(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if render:\n",
    "            print(f\"Day {env.day}, Action: {action}, Reward: {reward:.2f}, Effectiveness: {info['effectiveness']}\")\n",
    "        \n",
    "        episode_data.append({\n",
    "            'day': env.day,\n",
    "            'action': action.copy(),\n",
    "            'reward': reward,\n",
    "            'roi': info['roi'],\n",
    "            'effectiveness': info['effectiveness'].copy()\n",
    "        })\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    return total_reward, episode_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Random Agent\n",
    "\n",
    "A random agent that allocates budget randomly across channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def random_policy(state):\n",
    "    \"\"\"Random allocation policy\"\"\"\n",
    "    action = np.random.rand(3)\n",
    "    return action\n",
    "\n",
    "# Run multiple episodes with random policy\n",
    "n_episodes = 10\n",
    "random_rewards = []\n",
    "random_data = []\n",
    "\n",
    "env = MarketingEnvironment()\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    reward, data = run_episode(env, random_policy)\n",
    "    random_rewards.append(reward)\n",
    "    random_data.append(data)\n",
    "    \n",
    "print(f\"Random Policy - Avg Total Reward: {np.mean(random_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Heuristic Agent\n",
    "\n",
    "A heuristic agent that allocates budget proportionally to the current effectiveness of each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class HeuristicPolicy:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_size = 7  # Day, 3 effectiveness values, 3 recent spend values\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        # Extract effectiveness values from state\n",
    "        effectiveness = state[1:4]  # Indices 1-3 contain channel effectiveness\n",
    "        \n",
    "        # Allocate budget proportionally to effectiveness\n",
    "        if np.sum(effectiveness) > 0:\n",
    "            allocation = effectiveness / np.sum(effectiveness)\n",
    "        else:\n",
    "            allocation = np.ones(3) / 3  # Equal allocation if all effectiveness is zero\n",
    "        \n",
    "        return allocation\n",
    "\n",
    "# Run episodes with heuristic policy\n",
    "env = MarketingEnvironment()\n",
    "heuristic_policy = HeuristicPolicy(env)\n",
    "\n",
    "heuristic_rewards = []\n",
    "heuristic_data = []\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    reward, data = run_episode(env, heuristic_policy)\n",
    "    heuristic_rewards.append(reward)\n",
    "    heuristic_data.append(data)\n",
    "    \n",
    "print(f\"Heuristic Policy - Avg Total Reward: {np.mean(heuristic_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the performance of our baseline agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_rewards(rewards, title):\n",
    "    \"\"\"Plot rewards across episodes\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_cumulative_rewards(data_list, labels):\n",
    "    \"\"\"Plot cumulative rewards over time for multiple agents\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Calculate mean cumulative rewards for each day across episodes\n",
    "    for i, data in enumerate(data_list):\n",
    "        # Average across episodes\n",
    "        daily_rewards = np.zeros(30)  # Assuming 30-day episodes\n",
    "        for episode_data in data:\n",
    "            for entry in episode_data:\n",
    "                daily_rewards[entry['day']-1] += entry['reward'] / len(data)\n",
    "        \n",
    "        # Calculate cumulative rewards\n",
    "        cumulative_rewards = np.cumsum(daily_rewards)\n",
    "        \n",
    "        plt.plot(range(1, 31), cumulative_rewards, label=labels[i], linewidth=2)\n",
    "    \n",
    "    plt.title('Cumulative Reward by Day')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot baseline results\n",
    "plot_rewards(random_rewards, 'Random Policy - Rewards per Episode')\n",
    "plot_rewards(heuristic_rewards, 'Heuristic Policy - Rewards per Episode')\n",
    "plot_cumulative_rewards([random_data, heuristic_data], ['Random', 'Heuristic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Policy Gradient Implementation\n",
    "\n",
    "Now we'll implement a policy gradient algorithm to learn an optimal marketing budget allocation strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Policy Network Architecture\n",
    "\n",
    "We'll use a simple neural network to represent our policy. The network will take the state as input and output parameters for a multivariate normal distribution over actions (budget allocations).\n",
    "\n",
    "For the marketing budget allocation problem:\n",
    "- Input: State representation (day, channel effectiveness, recent spending, pending conversions)\n",
    "- Output: Mean values for each channel's budget allocation (we'll use a fixed standard deviation)\n",
    "\n",
    "```\n",
    "             ┌───────────┐\n",
    "             │  State s  │\n",
    "             └─────┬─────┘\n",
    "                   │\n",
    "                   ▼\n",
    "        ┌─────────────────────┐\n",
    "        │ Dense Layer (64)    │\n",
    "        │ ReLU Activation     │\n",
    "        └─────────┬───────────┘\n",
    "                  │\n",
    "                  ▼\n",
    "        ┌─────────────────────┐\n",
    "        │ Dense Layer (32)    │\n",
    "        │ ReLU Activation     │\n",
    "        └─────────┬───────────┘\n",
    "                  │\n",
    "                  ▼\n",
    "        ┌─────────────────────┐\n",
    "        │ Output Layer        │\n",
    "        │ 3 Means (Sigmoid)   │\n",
    "        └─────────┬───────────┘\n",
    "                  │\n",
    "                  ▼\n",
    "        ┌─────────────────────┐\n",
    "        │ Normal Distribution │\n",
    "        │ N(μ, σ²)            │\n",
    "        └─────────┬───────────┘\n",
    "                  │\n",
    "                  ▼\n",
    "             ┌─────────┐\n",
    "             │ Action a │\n",
    "             └─────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Policy Gradient Loss Derivation\n",
    "\n",
    "The core of policy gradient methods is the policy gradient theorem, which gives us a way to compute the gradient of expected return J(θ) with respect to policy parameters θ:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = E_{\\tau \\sim \\pi_\\theta}[\\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot R_t]$$\n",
    "\n",
    "Where:\n",
    "- $\\tau$ is a trajectory (sequence of states, actions, rewards)\n",
    "- $\\pi_\\theta(a|s)$ is the policy (probability of taking action $a$ in state $s$)\n",
    "- $R_t$ is the return (sum of rewards from time $t$ to the end of episode)\n",
    "\n",
    "In practice, we use sample trajectories and compute the loss as:\n",
    "\n",
    "$$L(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=0}^{T} \\log \\pi_\\theta(a_t^i|s_t^i) \\cdot R_t^i$$\n",
    "\n",
    "For our continuous action space (budget allocations), we'll use a multivariate normal distribution for our policy. For simplicity, we'll use diagonal covariance (independent dimensions) with fixed standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, action_dim),\n",
    "            nn.Sigmoid()  # Output in [0,1] range for budget allocation\n",
    "        )\n",
    "        \n",
    "        # Log standard deviations for each action dimension\n",
    "        # We'll use fixed standard deviations for simplicity\n",
    "        self.log_std = nn.Parameter(torch.ones(action_dim) * -0.5)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        # Convert state to tensor if it's not already\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.FloatTensor(state)\n",
    "        \n",
    "        # Get mean actions\n",
    "        action_means = self.network(state)\n",
    "        \n",
    "        # Create action distribution\n",
    "        std = torch.exp(self.log_std)\n",
    "        return action_means, std\n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        \"\"\"Sample an action from the policy distribution\"\"\"\n",
    "        mean, std = self.forward(state)\n",
    "        \n",
    "        # Create normal distribution\n",
    "        normal = Normal(mean, std)\n",
    "        \n",
    "        # Sample action\n",
    "        action = normal.sample()\n",
    "        \n",
    "        # Calculate log probability\n",
    "        log_prob = normal.log_prob(action).sum(dim=-1)\n",
    "        \n",
    "        # Clip action to valid range [0, 1]\n",
    "        action = torch.clamp(action, 0, 1)\n",
    "        \n",
    "        return action.detach().numpy(), log_prob.detach()\n",
    "    \n",
    "    def evaluate_action(self, state, action):\n",
    "        \"\"\"Calculate log probability of an action\"\"\"\n",
    "        mean, std = self.forward(state)\n",
    "        \n",
    "        # Create normal distribution\n",
    "        normal = Normal(mean, std)\n",
    "        \n",
    "        # Calculate log probability\n",
    "        log_prob = normal.log_prob(action).sum(dim=-1)\n",
    "        \n",
    "        # Calculate entropy for exploration encouragement\n",
    "        entropy = normal.entropy().sum(dim=-1)\n",
    "        \n",
    "        return log_prob, entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Training Loop\n",
    "\n",
    "Now we'll implement the REINFORCE algorithm (vanilla policy gradient) to train our policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def collect_trajectory(env, policy):\n",
    "    \"\"\"Collect one trajectory (episode) using the policy\"\"\"\n",
    "    state = env.reset()\n",
    "    states, actions, rewards, log_probs = [], [], [], []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Convert state to tensor\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        \n",
    "        # Sample action from policy\n",
    "        action, log_prob = policy.sample_action(state_tensor)\n",
    "        \n",
    "        # Take action in environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Store experience\n",
    "        states.append(state_tensor)\n",
    "        actions.append(torch.FloatTensor(action))\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        # Update state and episode reward\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "    \n",
    "    return states, actions, rewards, log_probs, episode_reward\n",
    "\n",
    "def compute_returns(rewards, gamma=0.99):\n",
    "    \"\"\"Compute discounted returns\"\"\"\n",
    "    returns = []\n",
    "    R = 0\n",
    "    \n",
    "    # Calculate returns from the end of the episode\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    \n",
    "    # Normalize returns for stability\n",
    "    returns = torch.tensor(returns)\n",
    "    if len(returns) > 1:\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
    "    \n",
    "    return returns\n",
    "\n",
    "def train_policy_gradient(env, policy, optimizer, num_episodes=200, gamma=0.99, print_every=10, entropy_weight=0.01):\n",
    "    \"\"\"Train policy using policy gradient\"\"\"\n",
    "    episode_rewards = []\n",
    "    average_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        # Collect trajectory\n",
    "        states, actions, rewards, log_probs, episode_reward = collect_trajectory(env, policy)\n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # Compute returns\n",
    "        returns = compute_returns(rewards, gamma)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions)\n",
    "        \n",
    "        # Compute loss\n",
    "        # We need to recompute log probabilities for gradient calculation\n",
    "        log_probs, entropy = policy.evaluate_action(states, actions)\n",
    "        \n",
    "        # Policy gradient loss\n",
    "        policy_loss = -(log_probs * returns).mean()\n",
    "        \n",
    "        # Add entropy bonus to encourage exploration\n",
    "        loss = policy_loss - entropy_weight * entropy.mean()\n",
    "        \n",
    "        # Update policy\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track progress\n",
    "        if episode % print_every == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-print_every:])\n",
    "            average_rewards.append(avg_reward)\n",
    "            print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}\")\n",
    "    \n",
    "    return episode_rewards, average_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Train the Policy Gradient Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create environment\n",
    "env = MarketingEnvironment()\n",
    "\n",
    "# Initialize policy network\n",
    "state_dim = len(env.reset())\n",
    "action_dim = env.num_channels\n",
    "policy = PolicyNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "# Train policy\n",
    "episode_rewards, average_rewards = train_policy_gradient(\n",
    "    env, policy, optimizer, \n",
    "    num_episodes=100,  # Reduced for notebook runtime\n",
    "    print_every=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Evaluate and Compare Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create policy function that uses trained policy network\n",
    "def pg_policy(state):\n",
    "    action, _ = policy.sample_action(state)\n",
    "    return action\n",
    "\n",
    "# Run episodes with trained policy\n",
    "pg_rewards = []\n",
    "pg_data = []\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    reward, data = run_episode(env, pg_policy)\n",
    "    pg_rewards.append(reward)\n",
    "    pg_data.append(data)\n",
    "    \n",
    "print(f\"Policy Gradient - Avg Total Reward: {np.mean(pg_rewards):.2f}\")\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(episode_rewards)\n",
    "plt.plot(range(0, len(episode_rewards), 10), average_rewards, 'r--')\n",
    "plt.title('Policy Gradient Training Progress')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.legend(['Episode Reward', 'Average Reward (10 episodes)'])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compare all agents\n",
    "plot_cumulative_rewards([random_data, heuristic_data, pg_data], ['Random', 'Heuristic', 'Policy Gradient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how our trained policy allocates budget across channels compared to the baseline policies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run a single episode with each policy and plot the budget allocations\n",
    "def plot_budget_allocations(policies, policy_names):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, (policy_fn, name) in enumerate(zip(policies, policy_names)):\n",
    "        # Run episode\n",
    "        env = MarketingEnvironment()\n",
    "        _, data = run_episode(env, policy_fn)\n",
    "        \n",
    "        # Extract allocations\n",
    "        days = [entry['day'] for entry in data]\n",
    "        ch1_allocations = [entry['action'][0] for entry in data]\n",
    "        ch2_allocations = [entry['action'][1] for entry in data]\n",
    "        ch3_allocations = [entry['action'][2] for entry in data]\n",
    "        \n",
    "        # Extract effectiveness for reference\n",
    "        ch1_effectiveness = [entry['effectiveness'][0] for entry in data]\n",
    "        ch2_effectiveness = [entry['effectiveness'][1] for entry in data]\n",
    "        ch3_effectiveness = [entry['effectiveness'][2] for entry in data]\n",
    "        \n",
    "        # Plot allocations\n",
    "        plt.subplot(len(policies), 2, 2*i+1)\n",
    "        plt.stackplot(days, ch1_allocations, ch2_allocations, ch3_allocations, \n",
    "                      labels=['Email', 'Social', 'Search'],\n",
    "                      alpha=0.7)\n",
    "        plt.title(f'{name} - Budget Allocation')\n",
    "        plt.xlabel('Day')\n",
    "        plt.ylabel('Allocation Proportion')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Plot channel effectiveness\n",
    "        plt.subplot(len(policies), 2, 2*i+2)\n",
    "        plt.plot(days, ch1_effectiveness, 'g-', label='Email')\n",
    "        plt.plot(days, ch2_effectiveness, 'b-', label='Social')\n",
    "        plt.plot(days, ch3_effectiveness, 'r-', label='Search')\n",
    "        plt.title(f'{name} - Channel Effectiveness')\n",
    "        plt.xlabel('Day')\n",
    "        plt.ylabel('Effectiveness')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot allocations\n",
    "plot_budget_allocations(\n",
    "    [random_policy, heuristic_policy, pg_policy], \n",
    "    ['Random Policy', 'Heuristic Policy', 'Policy Gradient']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpretation & Marketing Insights\n",
    "\n",
    "Now that we've trained our policy gradient agent, let's analyze what marketing insights we can derive from its learned budget allocation strategy.\n",
    "\n",
    "Our policy gradient agent has learned several key behaviors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Run a detailed analysis of a single episode with our trained policy\n",
    "env = MarketingEnvironment()\n",
    "_, detailed_data = run_episode(env, pg_policy, render=False)\n",
    "\n",
    "# Extract data for analysis\n",
    "days = [entry['day'] for entry in detailed_data]\n",
    "allocations = np.array([entry['action'] for entry in detailed_data])\n",
    "effectiveness = np.array([entry['effectiveness'] for entry in detailed_data])\n",
    "rewards = [entry['reward'] for entry in detailed_data]\n",
    "rois = [entry['roi'] for entry in detailed_data]\n",
    "\n",
    "# Plot various metrics\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Plot allocations vs. effectiveness\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.stackplot(days, allocations[:, 0], allocations[:, 1], allocations[:, 2],\n",
    "             labels=['Email', 'Social', 'Search'], alpha=0.7)\n",
    "plt.title('Budget Allocation Strategy')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Allocation Proportion')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(days, effectiveness[:, 0], 'g-', label='Email')\n",
    "plt.plot(days, effectiveness[:, 1], 'b-', label='Social')\n",
    "plt.plot(days, effectiveness[:, 2], 'r-', label='Search')\n",
    "plt.title('Channel Effectiveness')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Effectiveness')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot daily rewards and ROI\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(days, rewards, 'b-')\n",
    "plt.title('Daily Reward')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(days, rois, 'g-')\n",
    "plt.title('Daily ROI')\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('ROI')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation between effectiveness and allocation\n",
    "ch1_corr = np.corrcoef(effectiveness[:, 0], allocations[:, 0])[0, 1]\n",
    "ch2_corr = np.corrcoef(effectiveness[:, 1], allocations[:, 1])[0, 1]\n",
    "ch3_corr = np.corrcoef(effectiveness[:, 2], allocations[:, 2])[0, 1]\n",
    "\n",
    "print(f\"Correlation between effectiveness and allocation:\")\n",
    "print(f\"Email: {ch1_corr:.2f}\")\n",
    "print(f\"Social: {ch2_corr:.2f}\")\n",
    "print(f\"Search: {ch3_corr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Marketing Insights\n",
    "\n",
    "From the trained policy behavior, we can derive several marketing insights:\n",
    "\n",
    "1. **Dynamic Budget Reallocation**: The policy gradient agent has learned to dynamically adjust budget allocations based on changing channel effectiveness, unlike the static or simple heuristic strategies.\n",
    "\n",
    "2. **Exploitation vs. Exploration**: The agent balances between exploiting channels known to be effective and exploring potentially underutilized channels, ensuring it doesn't miss opportunities due to the stochastic nature of returns.\n",
    "\n",
    "3. **Response to Diminishing Returns**: The agent appears to recognize diminishing returns in channels, reducing allocation when oversaturation occurs rather than continuing to pour budget into a single high-performing channel.\n",
    "\n",
    "4. **Anticipation of Delayed Conversions**: Unlike simple heuristics that might react only to immediate returns, the policy seems to account for the delayed conversion model, making decisions that optimize for long-term cumulative reward.\n",
    "\n",
    "5. **Channel Synergies**: The policy may have learned implicit relationships between channels - for example, how email effectiveness might influence subsequent social media engagement.\n",
    "\n",
    "For marketers, this suggests that:\n",
    "- Budget allocation should be reviewed and adjusted more frequently than typical monthly or quarterly cycles\n",
    "- Channel performance should be evaluated in the context of the full customer journey, not just immediate returns\n",
    "- Testing low-allocation channels periodically can reveal changing effectiveness patterns\n",
    "- Sophisticated adaptive strategies can significantly outperform static allocation plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercises\n",
    "\n",
    "### Exercise 1: Vary Reward Delay and Noise\n",
    "\n",
    "Modify the environment to test how different reward delay patterns and noise levels affect the learning stability of the policy gradient agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Modify the delay_probs and volatility parameters in the MarketingEnvironment class\n",
    "# Train new policies with these modified environments, then compare performance\n",
    "\n",
    "# Example:\n",
    "# 1. Create an environment with longer delay\n",
    "# 2. Create an environment with higher volatility\n",
    "# 3. Train policies on each and compare learning curves and final performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Add a Spend Cap Constraint\n",
    "\n",
    "Modify the policy network to respect a total daily budget cap while still optimizing allocation across channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Modify the PolicyNetwork class to ensure the sum of allocations respects a budget cap\n",
    "# Hint: You can use a softmax output layer instead of sigmoid to ensure allocations sum to 1,\n",
    "# or you can normalize the output in the action sampling method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Alternative Reward Metrics\n",
    "\n",
    "Modify the environment to use customer lifetime value (LTV) or another marketing metric as the reward signal instead of immediate revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Modify the MarketingEnvironment class to include a customer LTV model\n",
    "# For example, you could add a customer state that tracks repeated interactions\n",
    "# or implement a simplified LTV calculation based on first purchase and retention probability\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Further Reading & Citations\n",
    "\n",
    "### Advanced Reinforcement Learning for Marketing\n",
    "\n",
    "- **Actor-Critic Methods**: Combines value-based and policy-based methods for greater stability and sample efficiency\n",
    "- **Offline RL**: Learn from historical marketing data without active experimentation\n",
    "- **Multi-Objective RL**: Optimize for multiple marketing KPIs simultaneously (e.g., revenue, customer acquisition cost, retention)\n",
    "- **Contextual Bandits**: Simpler special case of RL focused on immediate reward optimization\n",
    "\n",
    "### Key Papers & Resources\n",
    "\n",
    "1. Silver, D., et al. (2014). [\"Deterministic Policy Gradient Algorithms\"](http://proceedings.mlr.press/v32/silver14.pdf)\n",
    "2. Schulman, J., et al. (2017). [\"Proximal Policy Optimization Algorithms\"](https://arxiv.org/abs/1707.06347)\n",
    "3. Theocharous, G., et al. (2015). [\"Personalized Ad Recommendation Systems for Life-Time Value Optimization with Guarantees\"](https://www.ijcai.org/Proceedings/15/Papers/081.pdf)\n",
    "4. Ie, E., et al. (2019). [\"SlateQ: A Tractable Decomposition for Reinforcement Learning with Recommendation Sets\"](https://arxiv.org/abs/1905.12767)\n",
    "5. Rohde, D., et al. (2018). [\"ReAgent: A Toolkit for Applied Reinforcement Learning\"](https://ai.facebook.com/blog/reagent-applying-reinforcement-learning-to-real-world-problems/)\n",
    "\n",
    "### Books\n",
    "1. Sutton, R. S., & Barto, A. G. (2018). [\"Reinforcement Learning: An Introduction\"](http://incompleteideas.net/book/the-book-2nd.html)\n",
    "\n",
    "### Online Courses\n",
    "1. [\"Practical Reinforcement Learning\"](https://www.coursera.org/learn/practical-rl) (Coursera)\n",
    "2. [\"Deep Reinforcement Learning\"](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) (Udacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Actor-Critic Implementation**: Extend this notebook with an actor-critic approach for more stable learning\n",
    "- **Offline RL with Historical Data**: Adapt the algorithms to learn from historical marketing campaign data\n",
    "- **A/B Testing Integration**: Design a system that combines RL with traditional A/B testing for safer deployment\n",
    "- **Multi-Channel Attribution**: Incorporate attribution modeling to better distribute credit for conversions\n",
    "- **Bayesian RL**: Add uncertainty estimates to guide exploration and provide confidence intervals on budget decisions\n",
    "- **User Segmentation**: Extend the environment to handle different user segments with varying response patterns"
   ]
  }  
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}